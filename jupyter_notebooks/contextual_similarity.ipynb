{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "36c1b1c4f67c8086"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:35:58.024885Z",
     "start_time": "2025-10-29T17:35:47.313011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load a pre‑trained model\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "paragraph_one = \"\"\"(48) Considering that the financial risks associated with the support\n",
    " to Moldova in the form of loans under the Facility\n",
    " are comparable to the financial risks associated with lending operations\n",
    " under Regulation (EU) 2021/947, provisioning for the financial liability from\n",
    " loans under this Regulation should be constituted at the rate\n",
    " of 9%, in line with Article 214 of the Financial\n",
    " Regulation and the funding of the provisioning should be sourced\n",
    " from the envelope allocated to the Neighbourhood geographic programme under\n",
    " Article [[6(2),]] point (a), of Regulation (EU) [[2021/947.]]\"\"\"\n",
    "\n",
    "paragraph_two = \"\"\"(48) Tā kā finanšu risks, kas saistīts ar atbalstu, kurš\n",
    " atbilstīgi mehānismam sniegts Moldovai aizdevumu veidā, ir līdzīgs finanšu riskam,\n",
    " kas saistīts ar Regulas (ER) 2021/947 ietvaros veiktajām aizdevumu operācijām,\n",
    " uzkrājumi to finanšu saistību segšanai, kas izriet no šīs regulas\n",
    " ietvaros sniegtajiem aizdevumiem, būtu jāveido ar likmi 9% saskaņā ar\n",
    " Finanšu regulas 214. pantu, un uzkrājumu finansējums būtu jāiegūst no\n",
    " finansējuma, kas piešķirts Kaimiņattiecību ģeogrāfiskajai programmai saskaņā ar Regulas (ES)\n",
    " [[2021/9476.]] panta 2. punkta q) apakšpunktu.\"\"\"\n",
    "\n",
    "# Use the full paragraphs (or split into sentences if needed)\n",
    "sentence1 = paragraph_one\n",
    "sentence2 = paragraph_two\n",
    "\n",
    "# 2. Encode sentences as embeddings\n",
    "emb1 = model.encode(sentence1)\n",
    "emb2 = model.encode(sentence2)\n",
    "\n",
    "# 3. Compute cosine similarity\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "sim_score = cosine_similarity(emb1, emb2)\n",
    "print(f\"Similarity score between sentences: {sim_score:.4f}\")\n",
    "\n",
    "# 4. Decide a threshold for \"sufficient similarity\"\n",
    "threshold = 0.95\n",
    "if sim_score >= threshold:\n",
    "    print(\"✅ Sentences appear to have the same meaning (above threshold).\")\n",
    "else:\n",
    "    print(\"⚠️ Sentences might differ in meaning (below threshold).\")\n"
   ],
   "id": "d89a31b16537e916",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robbe\\.conda\\envs\\DataScienceEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between sentences: 0.9203\n",
      "⚠️ Sentences might differ in meaning (below threshold).\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:39:27.565760Z",
     "start_time": "2025-10-29T17:37:50.811336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import stanza\n",
    "\n",
    "# Download and initialize models\n",
    "stanza.download('en')  # English\n",
    "stanza.download('lv')  # Latvian\n",
    "nlp_en = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "nlp_lv = stanza.Pipeline('lv', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "doc1 = nlp_en(paragraph_one)\n",
    "doc2 = nlp_lv(paragraph_two)\n",
    "\n",
    "# Extract subjects\n",
    "def extract_subjects_stanza(doc):\n",
    "    subjects = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.deprel in (\"nsubj\", \"nsubj:pass\", \"csubj\"):\n",
    "                subjects.append(word.text)\n",
    "    return subjects\n",
    "\n",
    "subjects1 = extract_subjects_stanza(doc1)\n",
    "subjects2 = extract_subjects_stanza(doc2)\n",
    "print(subjects1)\n",
    "print(subjects2)\n"
   ],
   "id": "c4b912de0af648d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 48.0MB/s]                    \n",
      "2025-10-29 18:37:55 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:37:55 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/default.zip: 100%|██████████| 526M/526M [00:51<00:00, 10.2MB/s] \n",
      "2025-10-29 18:38:48 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\en\\default.zip\n",
      "2025-10-29 18:38:54 INFO: Finished downloading models and saved to C:\\Users\\robbe\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.11MB/s]                    \n",
      "2025-10-29 18:38:54 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:38:54 INFO: Downloading default packages for language: lv (Latvian) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-lv/resolve/v1.11.0/models/default.zip: 100%|██████████| 213M/213M [00:18<00:00, 11.2MB/s] \n",
      "2025-10-29 18:39:14 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\lv\\default.zip\n",
      "2025-10-29 18:39:16 INFO: Finished downloading models and saved to C:\\Users\\robbe\\stanza_resources\n",
      "2025-10-29 18:39:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 20.2MB/s]                    \n",
      "2025-10-29 18:39:16 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:39:16 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-10-29 18:39:16 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2025-10-29 18:39:16 INFO: Using device: cpu\n",
      "2025-10-29 18:39:16 INFO: Loading: tokenize\n",
      "2025-10-29 18:39:17 INFO: Loading: mwt\n",
      "2025-10-29 18:39:17 INFO: Loading: pos\n",
      "2025-10-29 18:39:20 INFO: Loading: lemma\n",
      "2025-10-29 18:39:20 INFO: Loading: depparse\n",
      "2025-10-29 18:39:21 INFO: Done loading processors!\n",
      "2025-10-29 18:39:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 16.9MB/s]                    \n",
      "2025-10-29 18:39:21 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:39:21 INFO: Loading these models for language: lv (Latvian):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | lvtb          |\n",
      "| pos       | lvtb_nocharlm |\n",
      "| lemma     | lvtb_nocharlm |\n",
      "| depparse  | lvtb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-10-29 18:39:21 INFO: Using device: cpu\n",
      "2025-10-29 18:39:21 INFO: Loading: tokenize\n",
      "2025-10-29 18:39:21 INFO: Loading: pos\n",
      "2025-10-29 18:39:23 INFO: Loading: lemma\n",
      "2025-10-29 18:39:24 INFO: Loading: depparse\n",
      "2025-10-29 18:39:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['risks', 'provisioning']\n",
      "['kas', 'kurš', 'kas', 'uzkrājumi', 'kas', 'kas']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:48:36.384792Z",
     "start_time": "2025-10-29T17:48:16.799952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import stanza\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Initialize Stanza pipelines\n",
    "# -----------------------------\n",
    "stanza.download('en')\n",
    "stanza.download('lv')\n",
    "\n",
    "nlp_en = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "nlp_lv = stanza.Pipeline('lv', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Paragraph examples\n",
    "# -----------------------------\n",
    "paragraph_en = \"Article [[6(2),]] point (a), of Regulation (EU) [[2021/947.]]\"\n",
    "paragraph_lv = \"Regulas (ES) [[2021/9476.]] panta 2. punkta q) apakšpunktu.\"\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Extract references with Stanza\n",
    "# -----------------------------\n",
    "def extract_references_stanza(doc, heads):\n",
    "    refs = []\n",
    "    for s in doc.sentences:\n",
    "        for w in s.words:\n",
    "            if w.text.lower() in heads:\n",
    "                # Include token itself + all children in the dependency tree\n",
    "                subtree_words = [ww.text for ww in s.words if ww.head == w.id or ww.id == w.id]\n",
    "                refs.append(\" \".join(subtree_words))\n",
    "    return refs\n",
    "\n",
    "refs_en = extract_references_stanza(nlp_en(paragraph_en), [\"article\", \"point\", \"regulation\"])\n",
    "refs_lv = extract_references_stanza(nlp_lv(paragraph_lv), [\"panta\", \"punkta\", \"regulas\"])\n",
    "\n",
    "print(\"English references:\", refs_en)\n",
    "print(\"Latvian references:\", refs_lv)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Normalize references (optional)\n",
    "# -----------------------------\n",
    "def normalize_ref(ref):\n",
    "    ref = ref.lower()\n",
    "    ref = re.sub(r\"[^\\w\\s]\", \"\", ref)  # remove punctuation\n",
    "    return ref.strip()\n",
    "\n",
    "refs_en_norm = [normalize_ref(r) for r in refs_en]\n",
    "refs_lv_norm = [normalize_ref(r) for r in refs_lv]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Encode references with SentenceTransformer\n",
    "# -----------------------------\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "emb_en = model.encode(refs_en_norm)\n",
    "emb_lv = model.encode(refs_lv_norm)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Compute pairwise cosine similarity\n",
    "# -----------------------------\n",
    "similarity_matrix = cosine_similarity(emb_en, emb_lv)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Align references by highest similarity\n",
    "# -----------------------------\n",
    "matched_refs = []\n",
    "for i, en_ref in enumerate(refs_en):\n",
    "    j = np.argmax(similarity_matrix[i])\n",
    "    lv_ref = refs_lv[j]\n",
    "    sim_score = similarity_matrix[i][j]\n",
    "\n",
    "    # Optional: Levenshtein distance for numeric/letter check\n",
    "    lev_distance = Levenshtein.distance(normalize_ref(en_ref), normalize_ref(lv_ref))\n",
    "\n",
    "    matched_refs.append({\n",
    "        \"english\": en_ref,\n",
    "        \"latvian\": lv_ref,\n",
    "        \"similarity\": sim_score,\n",
    "        \"levenshtein\": lev_distance,\n",
    "        \"flag_mismatch\": sim_score < 0.85 or lev_distance > 2\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Display results\n",
    "# -----------------------------\n",
    "for m in matched_refs:\n",
    "    print(f\"{m['english']!r} ↔ {m['latvian']!r} | similarity={m['similarity']:.3f} \"\n",
    "          f\"| Levenshtein={m['levenshtein']} | FLAG={m['flag_mismatch']}\")\n"
   ],
   "id": "19241b93fcd50489",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 48.0MB/s]                    \n",
      "2025-10-29 18:48:17 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:48:17 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-10-29 18:48:18 INFO: File exists: C:\\Users\\robbe\\stanza_resources\\en\\default.zip\n",
      "2025-10-29 18:48:22 INFO: Finished downloading models and saved to C:\\Users\\robbe\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 24.2MB/s]                    \n",
      "2025-10-29 18:48:22 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:48:22 INFO: Downloading default packages for language: lv (Latvian) ...\n",
      "2025-10-29 18:48:23 INFO: File exists: C:\\Users\\robbe\\stanza_resources\\lv\\default.zip\n",
      "2025-10-29 18:48:24 INFO: Finished downloading models and saved to C:\\Users\\robbe\\stanza_resources\n",
      "2025-10-29 18:48:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 8.09MB/s]                    \n",
      "2025-10-29 18:48:24 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:48:24 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-10-29 18:48:25 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2025-10-29 18:48:25 INFO: Using device: cpu\n",
      "2025-10-29 18:48:25 INFO: Loading: tokenize\n",
      "2025-10-29 18:48:25 INFO: Loading: mwt\n",
      "2025-10-29 18:48:25 INFO: Loading: pos\n",
      "2025-10-29 18:48:27 INFO: Loading: lemma\n",
      "2025-10-29 18:48:28 INFO: Loading: depparse\n",
      "2025-10-29 18:48:29 INFO: Done loading processors!\n",
      "2025-10-29 18:48:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 37.7MB/s]                    \n",
      "2025-10-29 18:48:29 INFO: Downloaded file to C:\\Users\\robbe\\stanza_resources\\resources.json\n",
      "2025-10-29 18:48:29 INFO: Loading these models for language: lv (Latvian):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | lvtb          |\n",
      "| pos       | lvtb_nocharlm |\n",
      "| lemma     | lvtb_nocharlm |\n",
      "| depparse  | lvtb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-10-29 18:48:29 INFO: Using device: cpu\n",
      "2025-10-29 18:48:29 INFO: Loading: tokenize\n",
      "2025-10-29 18:48:29 INFO: Loading: pos\n",
      "2025-10-29 18:48:31 INFO: Loading: lemma\n",
      "2025-10-29 18:48:32 INFO: Loading: depparse\n",
      "2025-10-29 18:48:33 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English references: ['Article 6 point', 'point a Regulation 2021/947', ', of Regulation EU']\n",
      "Latvian references: ['Regulas ES 2021', 'Regulas panta', 'panta 2. punkta']\n",
      "'Article 6 point' ↔ 'panta 2. punkta' | similarity=0.652 | Levenshtein=11 | FLAG=True\n",
      "'point a Regulation 2021/947' ↔ 'Regulas ES 2021' | similarity=0.600 | Levenshtein=15 | FLAG=True\n",
      "', of Regulation EU' ↔ 'Regulas ES 2021' | similarity=0.432 | Levenshtein=11 | FLAG=True\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "adea30f92e75c732"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
